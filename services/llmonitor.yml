services:
  litellm:
    networks:
      - monitoring
      - proxy
    image: ghcr.io/berriai/litellm:main-stable
    #########################################
    ## Uncomment these lines to start proxy with a config.yaml file ##
    # volumes:
    #  - ./config.yaml:/app/config.yaml <<- this is missing in the docker-compose file currently
    # command:
    #  - "--config=/app/config.yaml"
    ##############################################
    ports:
      - "4400:4000" # Map the container port to the host, change the host port if necessary
    environment:
      DATABASE_URL: "postgresql://llmproxy:dbpassword9090@llmdb:5432/litellm"
      STORE_MODEL_IN_DB: "True" # allows adding models to proxy via UI
      LITELLM_MASTER_KEY: "your_master_key_here"
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        max_attempts: 5
        delay: 10s
    depends_on:
      - llmdb # Indicates that this service depends on the 'db' service, ensuring 'db' starts first
    healthcheck: # Defines the health check configuration for the container
      test: [
          "CMD-SHELL",
          "wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1",
        ] # Command to execute for health check
      interval: 30s # Perform health check every 30 seconds
      timeout: 10s # Health check command times out after 10 seconds
      retries: 3 # Retry up to 3 times if health check fails
      start_period: 40s # Wait 40 seconds after container start before beginning health checks

  llmdb:
    networks:
      - monitoring
    image: postgres:16
    environment:
      POSTGRES_DB: litellm
      POSTGRES_USER: llmproxy
      POSTGRES_PASSWORD: dbpassword9090
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        max_attempts: 5
        delay: 10s
    expose:
      - "5432"
    volumes:
      - /mnt/apps/shared/litellm/data:/var/lib/postgresql/data # Persists Postgres data across container restarts
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d litellm -U llmproxy"]
      interval: 1s
      timeout: 5s
      retries: 10

  prometheus:
    networks:
      - monitoring
    image: prom/prometheus
    volumes:
      - /mnt/apps/shared/prometheus:/prometheus
      - /mnt/apps/shared/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=15d"
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        max_attempts: 5
        delay: 10s

  grafana:
    networks:
      - monitoring
    image: grafana/grafana:12.0.2
    ports:
      - "4500:3000"
    volumes:
      - /mnt/apps/shared/grafana:/var/lib/grafana # Persists Grafana data across container restarts
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=password
      - GF_DATABASE_TYPE=postgres
      - GF_DATABASE_HOST=llmdb
      - GF_DATABASE_NAME=litellm
      - GF_DATABASE_URL=postgresql://llmproxy:dbpassword9090@llmdb:5432/litellm
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD-SHELL", "curl -f localhost:3000/api/health && echo 'ready'"]
      interval: 10s
      retries: 30
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        max_attempts: 5
        delay: 10s

networks:
  monitoring:
    external: true
  proxy:
    external: true
